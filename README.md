# METIS Graph Partitioning â€“ Higgs Twitter Dataset

![Graph Partitioning Visualization](https://via.placeholder.com/1200x400?text=METIS+Partitioning+Visualization)

This repository contains partitioned graph data derived from various interaction networks in the Higgs Twitter dataset, using METIS (gpmetis) for community detection and parallel influence analysis algorithms.

## ğŸ‘¥ Development Team
- **Muhammad Daniyal Aziz** - Parallel Algorithm Design
- **Haleema Tahir** - Graph Analysis & Visualization  
- **Muhammad Bilal Khawar** - Performance Optimization

## ğŸ“œ Research Basis
Implementation of the parallel social behavior-based algorithm from:  
[Parallel Social Behavior-Based Algorithm for Influential Users Identification](https://drive.google.com/file/d/1vp5he-8ogdPJRFPiT6BDGPVZoH9CEnAh/view?usp=sharing)

## ğŸ§  Parallelization Strategy

### Hybrid MPI-OpenMP Architecture
```mermaid
flowchart TD
    A[Input Graph] --> B(METIS Partitioning)
    B --> C{Community Processing}
    C -->|Intra-Community| D[MPI Nodes]
    C -->|Inter-Community| E[OpenMP Threads]
    D --> F[Local Influence Calculation]
    E --> G[Global Influence Propagation]
    F & G --> H[Ranked Influencers]
```
## ğŸ“ Dataset Overview
### Graph Partition Files (*.part.8)
- File Name	Description	Nodes	Edges
- higgs-retweet_network.graph.part.8	Retweet interactions	456,626	14,855,842
- higgs-mention_network.graph.part.8	Mention network	456,626	12,673,554
- higgs-reply_network.graph.part.8	Reply network	456,626	6,633,598


ğŸ§ª Running Influence Algorithms

 ğŸ”¸ Serial Version

âœ… **Compile:**
```bash
g++ -std=c++17 -O2 -o serial_influence serial_influence.cpp
````

ğŸš€ **Run:**

```bash
./serial_influence
```

### ğŸ”¸ Parallel Version (Beowulf Cluster in Docker)

ğŸ³ **Set Up Cluster:**

```bash
sudo docker pull i212498/mpiclone
git clone https://github.com/i212498/Beowulf-Cluster-Using-Docker.git
cd Beowulf-Cluster-Using-Docker
./makecluster.sh 8
```

ğŸ” **Inside Node 1:**

```bash
sudo docker exec -it node1 bash
cd /home/storage
```

ğŸ’¡ **Copy your code and input files from host into `/home/storage`.**

ğŸ›  **Compile MPI Version:**

```bash
mpic++ -std=c++17 -O2 -o run_mpi run_mpi.cpp
```

ğŸš€ **Run MPI Program:**

```bash
mpirun --hostfile machinefile -np 8 ./run_mpi
```

ğŸ§¹ **Clean Up:**

```bash
./deletecluster.sh 8
```

---

## â± Performance Analysis

### ğŸ§® Basic Timing

#### Serial:

```bash
time ./serial_influence
```

#### Parallel:

```bash
time mpirun --hostfile machinefile -np 8 ./run_mpi
```

### ğŸ” Profiling with gprof

#### ğŸ§° Compile with Flags:

* **Serial**

```bash
g++ -std=c++17 -O2 -pg -o serial_influence serial_influence.cpp
```

* **MPI**

```bash
mpic++ -std=c++17 -O2 -pg -o run_mpi run_mpi.cpp
```

#### ğŸƒ Run:

```bash
./serial_influence
# OR
mpirun --hostfile machinefile -np 8 ./run_mpi
```

#### ğŸ“Š Analyze:

```bash
gprof ./serial_influence gmon.out > serial_profile.txt
gprof ./run_mpi gmon.out > mpi_profile.txt
```

### ğŸ” MPI Profiling with mpiP

#### âš™ï¸ Compile:

```bash
mpic++ -std=c++17 -O2 -o run_mpi run_mpi.cpp -lmpiP -lm -lbfd -liberty -lunwind -lz
```

#### ğŸ“ˆ Run:

```bash
mpirun --hostfile machinefile -np 8 ./run_mpi
```

### ğŸ“ˆ TAU Performance System

âœ… **Load and Compile:**

```bash
module load tau
tau_cxx.sh -std=c++17 -O2 -o run_mpi run_mpi.cpp
```

ğŸš€ **Run and Analyze:**

```bash
mpirun --hostfile machinefile -np 8 ./run_mpi
paraprof --pack run_mpi.ppk
paraprof run_mpi.ppk
```

### ğŸ’¡ Intel VTune

#### ğŸ”¬ Collect Hotspots:

```bash
vtune -collect hotspots -result-dir vtune_results ./serial_influence
```

#### ğŸƒ OR for MPI:

```bash
mpirun --hostfile machinefile -np 8 vtune -collect hotspots -result-dir vtune_results ./run_mpi
```

#### ğŸ“Š View Report:

```bash
vtune -report summary -result-dir vtune_results
```

---

## ğŸ“Œ Key Metrics to Compare

| Metric         | Description                            |
| -------------- | -------------------------------------- |
| Execution Time | Compare serial vs parallel             |
| Speedup        | Speedup = Serial Time / Parallel Time  |
| Efficiency     | Efficiency = Speedup / # of processors |
| MPI Overhead   | From mpiP profiling                    |
| CPU Hotspots   | From gprof or VTune                    |
| Load Balancing | From TAU profiler                      |

```

```
